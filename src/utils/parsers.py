from bs4 import BeautifulSoup
import asyncio
import httpx
from typing import List
import os
import random
from playwright.async_api import async_playwright
 
def _extract_baidu_hot_search_links(html_content: str, n: int = 1) -> list[dict]:
    """
    ä»ç™¾åº¦çƒ­æœç­‰æœç´¢å¹³å°çš„ HTML ä¸­æå–å‰ n ä¸ªç»“æœçš„è¶…é“¾æ¥
    
    Args:
        html_content: ç½‘é¡µ HTML å†…å®¹
        n: éœ€è¦æå–çš„ç»“æœæ•°é‡ï¼Œé»˜è®¤ 1
        
    Returns:
        list[dict]: åŒ…å«æ ‡é¢˜å’Œé“¾æ¥çš„å­—å…¸åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{"title": "...", "url": "..."}]
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    
    result_blocks = soup.find_all('div', class_='result c-container xpath-log new-pmd', limit=n)

    print(f"[DEBUG] Found {len(result_blocks)} result blocks")

    links = []
    for block in result_blocks:
        # åœ¨æ¯ä¸ªå—ä¸­æ‰¾åˆ° <a> æ ‡ç­¾
        item = {}
        a_tag = block.find('a')
        img = block.find('img')
        if a_tag:
            item["url"] = a_tag['href'] # æå–æ–‡ç« é“¾æ¥
        if img and img.has_attr('src'):
            item["img"] = img['src'] # æå–å›¾ç‰‡é“¾æ¥
        links.append(item)

    print(f"[DEBUG] Extracted {len(links)} valid links")

    return links


HIT_WEBSITES = {
    "baidu": _extract_baidu_hot_search_links,
}

async def mining_from_serch_with_browser(platform: str, url: str, limit: int = 1, interactive: bool = True) -> List[dict]:
    """
    ä½¿ç”¨ Playwright æµè§ˆå™¨è‡ªåŠ¨åŒ–ä»æœç´¢é¡µé¢æå–é“¾æ¥ï¼Œå¯ç»•è¿‡éªŒè¯ç 
    
    Args:
        platform: å¹³å°åç§°ï¼ˆå¦‚ 'baidu'ï¼‰
        url: ç›®æ ‡ç½‘é¡µ URL
        limit: éœ€è¦æå–çš„ç»“æœæ•°é‡ï¼Œé»˜è®¤ 1
        interactive: æ˜¯å¦å¯ç”¨äº¤äº’å¼éªŒè¯ç å¤„ç†ï¼Œé»˜è®¤ True
        
    Returns:
        list[dict]: åŒ…å«æå–å†…å®¹çš„å­—å…¸åˆ—è¡¨
    """
    mining_results = []
    
    if platform not in HIT_WEBSITES:
        mining_results.append({"url": url})
        return mining_results
    
    async with async_playwright() as p:
        # å½“éœ€è¦äº¤äº’å¼éªŒè¯æ—¶ï¼Œä½¿ç”¨æœ‰å¤´æ¨¡å¼
        print(f"ğŸš€ å¯åŠ¨æµè§ˆå™¨ (headless={not interactive}, interactive={interactive})")
        browser = await p.chromium.launch(
            headless=not interactive,  # äº¤äº’æ¨¡å¼ä¸‹æ˜¾ç¤ºæµè§ˆå™¨çª—å£
            args=[
                '--disable-blink-features=AutomationControlled',
                '--no-sandbox',
                '--disable-dev-shm-usage',
            ],
            # å¢åŠ æ…¢é€Ÿæ¨¡å¼ï¼Œæ›´å®¹æ˜“è§‚å¯Ÿï¼ˆä»…è°ƒè¯•æ—¶ä½¿ç”¨ï¼‰
            # slow_mo=100 if interactive else 0,
        )
        
        # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·
        context = await browser.new_context(
            user_agent=os.getenv(
                "NEWSNOW_USER_AGENT",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/118.0 Safari/537.36",
            ),
            viewport={'width': 1920, 'height': 1080},
            locale='zh-CN',
            timezone_id='Asia/Shanghai',
        )
        
        # æ³¨å…¥ JavaScript éšè— webdriver ç‰¹å¾
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
        
        page = await context.new_page()
        
        try:
            # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
            await asyncio.sleep(random.uniform(1, 3))
            
            # è®¿é—®é¡µé¢ï¼Œç­‰å¾…ç½‘ç»œç©ºé—²
            print(f"ğŸŒ æ­£åœ¨è®¿é—®: {url}")
            try:
                await page.goto(url, wait_until='networkidle', timeout=30000)
            except Exception as e:
                # å³ä½¿è¶…æ—¶ä¹Ÿç»§ç»­ï¼Œå¯èƒ½åªæ˜¯éƒ¨åˆ†èµ„æºåŠ è½½å¤±è´¥
                print(f"âš ï¸  é¡µé¢åŠ è½½è­¦å‘Š: {e}")
                await page.wait_for_timeout(2000)
            
            # é¢å¤–ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            await page.wait_for_timeout(random.randint(2000, 4000))
            
            # æ£€æµ‹æ˜¯å¦é‡åˆ°éªŒè¯ç é¡µé¢ï¼ˆæ£€æŸ¥å¤šæ¬¡ä»¥ç¡®ä¿å‡†ç¡®ï¼‰
            max_retries = 3
            for retry in range(max_retries):
                current_url = page.url
                page_title = await page.title()
                
                print(f"ğŸ“ å½“å‰é¡µé¢ URL: {current_url}")
                print(f"ğŸ“„ å½“å‰é¡µé¢æ ‡é¢˜: {page_title}")
                
                # æ£€æµ‹ç™¾åº¦éªŒè¯ç ç‰¹å¾
                is_captcha_page = (
                    'captcha' in current_url.lower() or 
                    'tuxing' in current_url.lower() or
                    'wappass' in current_url.lower() or
                    'å®‰å…¨éªŒè¯' in page_title or
                    'ç™¾åº¦å®‰å…¨éªŒè¯' in page_title or
                    await page.locator('text=å®‰å…¨éªŒè¯').count() > 0
                )
                
                if is_captcha_page:
                    if interactive:
                        print("\n" + "="*60)
                        print("âš ï¸  æ£€æµ‹åˆ°ç™¾åº¦å®‰å…¨éªŒè¯ç ï¼")
                        print(f"ğŸ“ éªŒè¯ç é¡µé¢: {current_url}")
                        print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {page_title}")
                        print("="*60)
                        print("\nğŸ”” è¯·åœ¨å½“å‰æµè§ˆå™¨çª—å£ä¸­æ‰‹åŠ¨å®ŒæˆéªŒè¯...")
                        print("ğŸ’¡ æç¤ºï¼š")
                        print("   1. æŸ¥çœ‹æ‰“å¼€çš„ Chromium æµè§ˆå™¨çª—å£")
                        print("   2. å®Œæˆå›¾ç‰‡éªŒè¯æˆ–æ»‘å—éªŒè¯")
                        print("   3. ç­‰å¾…é¡µé¢è‡ªåŠ¨è·³è½¬åˆ°æœç´¢ç»“æœ")
                        print("   4. çœ‹åˆ°æ­£å¸¸æœç´¢ç»“æœåï¼Œå›åˆ°ç»ˆç«¯æŒ‰ Enter")
                        print("="*60 + "\n")
                        
                        # ç­‰å¾…ç”¨æˆ·è¾“å…¥
                        input("âœ‹ å®ŒæˆéªŒè¯åæŒ‰ Enter ç»§ç»­ >>> ")
                        
                        # ç”¨æˆ·å®ŒæˆéªŒè¯åï¼Œç­‰å¾…é¡µé¢ç¨³å®š
                        print("â³ ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ...")
                        await page.wait_for_timeout(3000)
                        
                        # å†æ¬¡æ£€æŸ¥æ˜¯å¦æˆåŠŸè·³è½¬
                        final_url = page.url
                        final_title = await page.title()
                        
                        print(f"ğŸ“ éªŒè¯å URL: {final_url}")
                        print(f"ğŸ“„ éªŒè¯åæ ‡é¢˜: {final_title}")
                        
                        # éªŒè¯æ˜¯å¦æˆåŠŸ
                        still_captcha = (
                            'captcha' in final_url.lower() or 
                            'tuxing' in final_url.lower() or
                            'wappass' in final_url.lower()
                        )
                        
                        if not still_captcha:
                            print("âœ… éªŒè¯æˆåŠŸï¼ç»§ç»­æå–å†…å®¹...")
                            break
                        else:
                            print("âš ï¸  ä¼¼ä¹è¿˜åœ¨éªŒè¯é¡µé¢...")
                            if retry < max_retries - 1:
                                print(f"ğŸ”„ é‡è¯•ç¬¬ {retry + 2}/{max_retries} æ¬¡...")
                                await page.wait_for_timeout(2000)
                            else:
                                print("âŒ éªŒè¯å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œè¿”å›åŸå§‹ URL")
                                mining_results.append({"url": url})
                                return mining_results
                    else:
                        print(f"âš ï¸  æ£€æµ‹åˆ°éªŒè¯ç ä½†æœªå¯ç”¨äº¤äº’æ¨¡å¼: {current_url}")
                        print("ğŸ’¡ æç¤º: è®¾ç½® interactive=True ä»¥æ‰‹åŠ¨å®ŒæˆéªŒè¯")
                        mining_results.append({"url": url})
                        return mining_results
                else:
                    # æ²¡æœ‰éªŒè¯ç ï¼Œç›´æ¥ç»§ç»­
                    print("âœ… æœªæ£€æµ‹åˆ°éªŒè¯ç ï¼Œç›´æ¥æå–å†…å®¹")
                    break
            
            # éšæœºæ»šåŠ¨é¡µé¢ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
            await page.evaluate('window.scrollBy(0, Math.random() * 500)')
            await asyncio.sleep(random.uniform(0.5, 1.5))
            
            # è·å–é¡µé¢å†…å®¹
            html_content = await page.content()
            
            # è°ƒè¯•ï¼šä¿å­˜ HTML å’Œæˆªå›¾ï¼ˆå¯é€‰ï¼‰
            if os.getenv('DEBUG_SAVE_HTML', 'true').lower() == 'true':
                os.makedirs('/tmp/crawler_debug', exist_ok=True)
                
                # ä¿å­˜ HTML
                debug_html_file = '/tmp/crawler_debug/baidu_debug.html'
                with open(debug_html_file, 'w', encoding='utf-8') as f:
                    f.write(html_content)
                print(f"[DEBUG] Saved HTML to {debug_html_file}")
                
                # ä¿å­˜æˆªå›¾
                debug_screenshot_file = '/tmp/crawler_debug/baidu_debug.png'
                await page.screenshot(path=debug_screenshot_file, full_page=True)
                print(f"[DEBUG] Saved screenshot to {debug_screenshot_file}")
            
            # ä½¿ç”¨å¯¹åº”å¹³å°çš„è§£æå™¨
            print(f"[DEBUG] Parsing HTML content, length: {len(html_content)} chars")
            extractor = HIT_WEBSITES[platform](html_content, n=limit)
            if extractor and len(extractor) > 0:
                mining_results.extend(extractor)
                print(f"âœ… Successfully extracted {len(extractor)} items")
            else:
                print(f"[WARNING] No content extracted from {url}, returning original URL")
                mining_results.append({"url": url})
                
        except Exception as e:
            print(f"âŒ Browser automation error for {url}: {e}")
            mining_results.append({"url": url})
        finally:
            await browser.close()
    
    return mining_results


def _is_security_challenge(html_content: str, response_url: str = "") -> bool:
    """
    æ£€æµ‹å“åº”æ˜¯å¦åŒ…å«å®‰å…¨éªŒè¯ï¼ˆéªŒè¯ç ã€åçˆ¬è™«æ£€æµ‹ï¼‰
    
    Args:
        html_content: å“åº”çš„ HTML å†…å®¹
        response_url: æœ€ç»ˆå“åº”çš„ URLï¼ˆå¯èƒ½è¢«é‡å®šå‘ï¼‰
        
    Returns:
        bool: å¦‚æœæ£€æµ‹åˆ°å®‰å…¨éªŒè¯è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    if not html_content:
        return False
    
    # æ£€æŸ¥ URL ç‰¹å¾
    url_lower = response_url.lower()
    url_indicators = [
        'captcha',
        'tuxing',
        'wappass',
        'verify',
        'challenge',
        'security',
    ]
    
    for indicator in url_indicators:
        if indicator in url_lower:
            print(f"ğŸ” æ£€æµ‹åˆ°å®‰å…¨éªŒè¯ URL ç‰¹å¾: {indicator}")
            return True
    
    # æ£€æŸ¥ HTML å†…å®¹ç‰¹å¾
    html_lower = html_content.lower()
    content_indicators = [
        'å®‰å…¨éªŒè¯',
        'ç™¾åº¦å®‰å…¨éªŒè¯',
        'æ»‘åŠ¨éªŒè¯',
        'å›¾å½¢éªŒè¯',
        'äººæœºéªŒè¯',
        'robot check',
        'security verification',
        'access denied',
        'captcha',
    ]
    
    for indicator in content_indicators:
        if indicator in html_lower:
            print(f"ğŸ” æ£€æµ‹åˆ°å®‰å…¨éªŒè¯å†…å®¹ç‰¹å¾: {indicator}")
            return True
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«éªŒè¯ç ç›¸å…³çš„è„šæœ¬æˆ–å…ƒç´ 
    if 'id="captcha"' in html_content or 'class="captcha"' in html_content:
        print("ğŸ” æ£€æµ‹åˆ°éªŒè¯ç å…ƒç´ ")
        return True
    
    return False


async def mining_from_serch(platform: str, client: httpx.AsyncClient, header: dict, url: str, limit: int = 1, use_browser: bool = False, interactive: bool = True) -> List[dict]:
    """
    ä»æœç´¢é¡µé¢ä¸­æå–é“¾æ¥ä¿¡æ¯
    ä¼˜åŒ–ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨å¿«é€Ÿçš„ HTTP è¯·æ±‚ï¼Œåªæœ‰åœ¨æ£€æµ‹åˆ°å®‰å…¨éªŒè¯æ—¶æ‰åˆ‡æ¢åˆ°æµè§ˆå™¨æ¨¡å¼

    Args:
        platform: å¹³å°åç§°
        client: HTTP å®¢æˆ·ç«¯
        header: è¯·æ±‚å¤´
        url: ç›®æ ‡ URL
        limit: æå–æ•°é‡é™åˆ¶
        use_browser: æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨æµè§ˆå™¨æ¨¡å¼ï¼ˆé»˜è®¤ Falseï¼Œè‡ªåŠ¨æ£€æµ‹ï¼‰
        interactive: æµè§ˆå™¨æ¨¡å¼ä¸‹æ˜¯å¦å¯ç”¨äº¤äº’å¼éªŒè¯ç å¤„ç†

    Returns:
        æå–çš„é“¾æ¥ä¿¡æ¯åˆ—è¡¨
    """
    mining_results = []

    if platform not in HIT_WEBSITES:
        mining_results.append({"url": url})
        return mining_results
    
    # å¦‚æœå¼ºåˆ¶ä½¿ç”¨æµè§ˆå™¨æ¨¡å¼ï¼Œç›´æ¥è°ƒç”¨æµè§ˆå™¨æ–¹æ³•
    if use_browser:
        print("ğŸŒ å¼ºåˆ¶ä½¿ç”¨æµè§ˆå™¨æ¨¡å¼")
        return await mining_from_serch_with_browser(platform, url, limit, interactive=interactive)
    
    # ç¬¬ä¸€æ­¥ï¼šå°è¯•ä½¿ç”¨å¿«é€Ÿçš„ HTTP è¯·æ±‚
    print(f"âš¡ å°è¯•å¿«é€Ÿ HTTP è¯·æ±‚: {url[:80]}...")
    
    # éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
    await asyncio.sleep(random.uniform(0.5, 1.5))
    
    # å¢å¼ºè¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
    enhanced_headers = {
        **header,
        "sec-ch-ua": '"Google Chrome";v="118", "Chromium";v="118", "Not=A?Brand";v="99"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Upgrade-Insecure-Requests": "1",
    }
    
    try:
        response = await client.get(url, headers=enhanced_headers, timeout=30)
        response.raise_for_status()
        html_content = response.text
        
        # æ£€æŸ¥æ˜¯å¦è§¦å‘äº†å®‰å…¨éªŒè¯
        final_url = str(response.url)
        if _is_security_challenge(html_content, final_url):
            # å¦‚æœ interactive ä¸º Falseï¼Œåˆ™ä¸åˆ‡æ¢åˆ°æµè§ˆå™¨æ¨¡å¼
            if not interactive:
                print("âš ï¸  æ£€æµ‹åˆ°å®‰å…¨éªŒè¯ï¼Œä½† interactive=Falseï¼Œè¿”å›åŸå§‹ URL")
                mining_results.append({"url": url})
                return mining_results
            
            print("âš ï¸  æ£€æµ‹åˆ°å®‰å…¨éªŒè¯ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°æµè§ˆå™¨æ¨¡å¼...")
            return await mining_from_serch_with_browser(platform, url, limit, interactive=interactive)
        
        # æœªæ£€æµ‹åˆ°å®‰å…¨éªŒè¯ï¼Œç»§ç»­ä½¿ç”¨ HTTP è¯·æ±‚æå–å†…å®¹
        print("âœ… HTTP è¯·æ±‚æˆåŠŸï¼Œæœªæ£€æµ‹åˆ°å®‰å…¨éªŒè¯")
        extractor = HIT_WEBSITES[platform](html_content, n=limit)
        
        if extractor and len(extractor) > 0:
            mining_results.extend(extractor)
            print(f"âœ… æˆåŠŸæå– {len(extractor)} æ¡å†…å®¹ (HTTP æ¨¡å¼)")
        else:
            print(f"âš ï¸  æœªæå–åˆ°å†…å®¹ï¼Œè¿”å›åŸå§‹ URL")
            mining_results.append({"url": url})
            
    except httpx.HTTPStatusError as e:
        # HTTP çŠ¶æ€é”™è¯¯ï¼Œå¯èƒ½æ˜¯åçˆ¬è™«æœºåˆ¶
        print(f"âš ï¸  HTTP è¯·æ±‚å¤±è´¥ (çŠ¶æ€ç  {e.response.status_code})ï¼Œåˆ‡æ¢åˆ°æµè§ˆå™¨æ¨¡å¼...")
        if interactive is False:
            print("âš ï¸  interactive=Falseï¼Œè¿”å›åŸå§‹ URL")
            mining_results.append({"url": url})
            return mining_results
        return await mining_from_serch_with_browser(platform, url, limit, interactive=interactive)
        
    except Exception as e:
        # å…¶ä»–é”™è¯¯ï¼ˆè¶…æ—¶ã€è¿æ¥å¤±è´¥ç­‰ï¼‰ï¼Œå°è¯•æµè§ˆå™¨æ¨¡å¼
        print(f"âš ï¸  HTTP è¯·æ±‚å¼‚å¸¸: {e}")
        print("ğŸ”„ åˆ‡æ¢åˆ°æµè§ˆå™¨æ¨¡å¼é‡è¯•...")
        return await mining_from_serch_with_browser(platform, url, limit, interactive=interactive)
    
    return mining_results


async def main():
    """Main entry point for testing."""
    test_url = "https://www.baidu.com/s?wd=%E5%B1%B1%E8%A5%BF%E5%90%95%E6%A2%81%E9%93%81%E8%B7%AF%E5%9D%8D%E5%A1%8C%E8%87%B411%E6%AD%BB%E7%B3%BB%E8%B0%A3%E8%A8%80"

    user_agent = os.getenv(
        "NEWSNOW_USER_AGENT",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/118.0 Safari/537.36",
    )
    accept_language = os.getenv("NEWSNOW_ACCEPT_LANGUAGE", "zh-CN,zh;q=0.9,en;q=0.8")
    referer = os.getenv("NEWSNOW_BASE_URL", "https://newsnow.busiyi.world").rstrip("/")
    cookie = os.getenv("NEWSNOW_COOKIE", "").strip()
    default_headers = {
        "User-Agent": user_agent,
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": accept_language,
        "Connection": "keep-alive",
        "Cache-Control": "no-cache",
        "Referer": referer,
    }
    if cookie:
        default_headers["Cookie"] = cookie

    async with httpx.AsyncClient(timeout=10, follow_redirects=True) as client:
        # print("æµ‹è¯•æ™®é€š HTTP è¯·æ±‚æ¨¡å¼ï¼š")
        # results = await mining_from_serch("baidu", client, default_headers, test_url, limit=1, use_browser=False)
        # for item in results:
        #     print(item)
        
        print("\næµ‹è¯•æµè§ˆå™¨è‡ªåŠ¨åŒ–æ¨¡å¼ï¼ˆæ¨èï¼Œå¯ç»•è¿‡éªŒè¯ç ï¼‰ï¼š")
        print("ğŸ’¡ å¦‚æœé‡åˆ°éªŒè¯ç ï¼Œæµè§ˆå™¨çª—å£ä¼šè‡ªåŠ¨æ‰“å¼€ï¼Œè¯·æ‰‹åŠ¨å®ŒæˆéªŒè¯\n")
        results_browser = await mining_from_serch("baidu", client, default_headers, test_url, limit=1, use_browser=False, interactive=False
                                                  )
        for item in results_browser:
            print(item)

if __name__ == "__main__":
    asyncio.run(main())