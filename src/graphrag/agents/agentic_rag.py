"""
Agentic RAG - å¤šç­–ç•¥æ™ºèƒ½æ£€ç´¢å¢å¼ºç”Ÿæˆ

ç‰¹ç‚¹ï¼š
1. å¤šç­–ç•¥å¹¶è¡Œæ£€ç´¢ - LocalSearch, GlobalSearch, HybridSearch, ChainOfExploration
2. æŸ¥è¯¢è§„åˆ’ - æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©æœ€ä½³ç­–ç•¥ç»„åˆ
3. ç»“æœèåˆ - å»é‡ã€æ’åºã€ç»¼åˆå¤šæºä¿¡æ¯
4. æµå¼è¾“å‡º - è¯¦ç»†æ‰§è¡Œæ—¥å¿—
"""

import json
import logging
import re
import time
from typing import List, Dict, Any, Optional, Generator, Set
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed

from langchain_core.messages import HumanMessage, SystemMessage

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """ç»Ÿä¸€çš„æœç´¢ç»“æœ"""
    strategy: str  # æ¥æºç­–ç•¥
    content: str   # ä¸»è¦å†…å®¹
    score: float = 1.0  # ç›¸å…³æ€§åˆ†æ•°
    source_entity: str = ""  # æ¥æºå®ä½“
    relation: str = ""  # å…³ç³»ç±»å‹
    metadata: Dict = field(default_factory=dict)


@dataclass
class AgentState:
    """Agent çŠ¶æ€"""
    question: str
    query_type: str = "general"  # general, entity, comparison, timeline, causal
    entities: List[str] = field(default_factory=list)
    strategies: List[str] = field(default_factory=list)
    results: List[SearchResult] = field(default_factory=list)
    final_answer: str = ""


# æ£€ç´¢åæ€ prompt
RETRIEVAL_REFLECT_PROMPT = """è¯„ä¼°æ£€ç´¢ç»“æœæ˜¯å¦è¶³å¤Ÿå›ç­”ç”¨æˆ·é—®é¢˜ã€‚

## ç”¨æˆ·é—®é¢˜
{question}

## å·²æ£€ç´¢åˆ°çš„ä¿¡æ¯æ‘˜è¦
{results_summary}

## è¯„ä¼°æ ‡å‡†ï¼ˆå®½æ¾ï¼‰
è¿™æ˜¯æ–°é—»çƒ­æœçŸ¥è¯†å›¾è°±ï¼Œä¸æ˜¯å­¦æœ¯æ•°æ®åº“ã€‚åªè¦æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶å³å¯åˆ¤å®šä¸º"è¶³å¤Ÿ"ï¼š
- æœ‰3æ¡ä»¥ä¸Šä¸é—®é¢˜ç›¸å…³çš„äº‹å®
- èƒ½å›ç­”é—®é¢˜çš„æ ¸å¿ƒéƒ¨åˆ†ï¼ˆå³ä½¿ç¼ºå°‘ç»†èŠ‚ï¼‰
- æåˆ°äº†é—®é¢˜ä¸­çš„å…³é”®å®ä½“

ä¸è¦æœŸæœ›çŸ¥è¯†å›¾è°±åŒ…å«ï¼š
- æƒå¨æœºæ„æŠ¥å‘Šï¼ˆWHOã€CDCç­‰ï¼‰
- ç²¾ç¡®çš„æ•°å­—æ•°æ®ï¼ˆç—…ä¾‹æ•°ã€æ—¥æœŸç­‰ï¼‰
- åª’ä½“åŸæ–‡é“¾æ¥

## è¾“å‡ºJSONæ ¼å¼
{{
    "sufficient": true/false,
    "reason": "ç®€çŸ­è¯´æ˜",
    "suggestion": "å¦‚æœä¸è¶³ï¼Œå»ºè®®è¡¥å……æ£€ç´¢çš„å…³é”®è¯ï¼ˆå¯é€‰ï¼‰"
}}

åªè¾“å‡ºJSONï¼š"""


# ç­”æ¡ˆåæ€ prompt
ANSWER_REFLECT_PROMPT = """è¯„ä¼°ç”Ÿæˆçš„ç­”æ¡ˆè´¨é‡ã€‚

## ç”¨æˆ·é—®é¢˜
{question}

## ç”Ÿæˆçš„ç­”æ¡ˆ
{answer}

## è¯„ä¼°è¦æ±‚
1. ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”äº†é—®é¢˜ï¼Ÿ
2. æ˜¯å¦æœ‰æ˜æ˜¾çš„äº‹å®é”™è¯¯æˆ–é€»è¾‘é—®é¢˜ï¼Ÿ
3. æ˜¯å¦è¿‡äºå†—é•¿æˆ–åç¦»ä¸»é¢˜ï¼Ÿ

## è¾“å‡ºJSONæ ¼å¼
{{
    "quality": "good/acceptable/poor",
    "issues": ["é—®é¢˜1", "é—®é¢˜2"],
    "suggestion": "å¦‚æœè´¨é‡å·®ï¼Œå»ºè®®å¦‚ä½•æ”¹è¿›"
}}

åªè¾“å‡ºJSONï¼š"""


# æŸ¥è¯¢åˆ†æ prompt
QUERY_ANALYZER_PROMPT = """åˆ†æç”¨æˆ·é—®é¢˜ï¼Œæå–å…³é”®ä¿¡æ¯ã€‚

é—®é¢˜ï¼š{question}

è¯·ç”¨JSONæ ¼å¼è¾“å‡ºï¼š
{{
    "query_type": "é—®é¢˜ç±»å‹(general/entity/comparison/timeline/causal)",
    "entities": ["é—®é¢˜ä¸­æåˆ°çš„å®ä½“"],
    "keywords": ["å…³é”®è¯"],
    "intent": "ç”¨æˆ·æ„å›¾ç®€è¿°"
}}

é—®é¢˜ç±»å‹è¯´æ˜ï¼š
- general: ä¸€èˆ¬æ€§é—®é¢˜ï¼Œå¦‚"æœ€è¿‘æœ‰ä»€ä¹ˆæ–°é—»"
- entity: å…³äºç‰¹å®šå®ä½“çš„é—®é¢˜ï¼Œå¦‚"é©¬æ–¯å…‹æœ€è¿‘åšäº†ä»€ä¹ˆ"
- comparison: æ¯”è¾ƒç±»é—®é¢˜ï¼Œå¦‚"Aå’ŒBæœ‰ä»€ä¹ˆå…³ç³»"
- timeline: æ—¶é—´çº¿é—®é¢˜ï¼Œå¦‚"æŸäº‹ä»¶çš„å‘å±•è¿‡ç¨‹"
- causal: å› æœåˆ†æé—®é¢˜ï¼Œå¦‚"ä¸ºä»€ä¹ˆä¼šå‘ç”Ÿ..."

åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š"""


SYNTHESIZER_PROMPT = """ä½ æ˜¯èµ„æ·±æ–°é—»åˆ†æå¸ˆã€‚æ ¹æ®æ£€ç´¢ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜ã€‚

## é—®é¢˜
{question}

## æ£€ç´¢ç»“æœ
{local_results}

{global_results}

{hybrid_results}

{chain_results}

## å›ç­”è¦æ±‚

1. **åˆ†ç±»æ•´ç†**ï¼šå°†ç›¸å…³æ–°é—»æŒ‰ä¸»é¢˜åˆ†ç±»ï¼ˆå¦‚æ”¿æ²»ã€ç»æµã€ç¤¾ä¼šã€å›½é™…ç­‰ï¼‰
2. **é€‚åº¦å±•å¼€**ï¼šæ¯æ¡é‡è¦æ–°é—»ç”¨2-3å¥è¯è¯´æ˜èƒŒæ™¯å’Œè¦ç‚¹ï¼Œä¸è¦åªåˆ—æ ‡é¢˜
3. **å…³è”åˆ†æ**ï¼šå¦‚æœå¤šæ¡æ–°é—»ä¹‹é—´æœ‰å…³è”ï¼ŒæŒ‡å‡ºå®ƒä»¬çš„è”ç³»
4. **è¿‡æ»¤å™ªå£°**ï¼šå¿½ç•¥æ˜æ˜¾ä¸ç›¸å…³çš„å†…å®¹ï¼Œä¸è¦å¼ºè¡Œæ‹¼å‡‘
5. **ç®€çŸ­é¢„æµ‹**ï¼šæœ€åå¯ä»¥ç”¨1-2å¥ç‚¹è¯„å½“å‰æ€åŠ¿æˆ–è¶‹åŠ¿

## æ ¼å¼ç¤ºä¾‹
```
**å›½é™…å…³ç³»**
å…³äºxxxäº‹ä»¶ï¼Œç›®å‰æƒ…å†µæ˜¯...èƒŒæ™¯æ˜¯...

**ç¤¾ä¼šæ°‘ç”Ÿ**
è¿‘æœŸxxxå¼•å‘å…³æ³¨ï¼Œå…·ä½“æ˜¯...

**è¶‹åŠ¿è§‚å¯Ÿ**
æ•´ä½“æ¥çœ‹...
```

## ç¦æ­¢
- ä¸è¦å†™æˆå­¦æœ¯æŠ¥å‘Šï¼ˆä¸éœ€è¦"é«˜ç½®ä¿¡åº¦""äº¤å‰éªŒè¯"ç­‰æœ¯è¯­ï¼‰
- ä¸è¦åªæ˜¯ç®€å•ç½—åˆ—äº‹ä»¶æ ‡é¢˜
- ä¿¡æ¯ä¸è¶³æ—¶ç›´æ¥è¯´æ˜ï¼Œä¸è¦ç¼–é€ 

è¯·å›ç­”ï¼š"""


class AgenticRAG:
    """
    å¤šç­–ç•¥ Agentic RAG å®ç°

    æ£€ç´¢ç­–ç•¥ï¼š
    1. LocalSearch - å®ä½“çº§ç²¾ç¡®æ£€ç´¢ï¼Œè·å–å®ä½“çš„ç›´æ¥å…³è”
    2. GlobalSearch - ç¤¾åŒºçº§å®è§‚æ£€ç´¢ï¼Œå¹¿æ³›è¯­ä¹‰æœç´¢
    3. HybridSearch - æ··åˆæ£€ç´¢ï¼Œç»“åˆå®ä½“å’Œè¯­ä¹‰
    4. ChainOfExploration - å¤šè·³å›¾è°±æ¢ç´¢
    """

    def __init__(self, zep_client=None, llm=None):
        self.zep_client = zep_client
        self.llm = llm
        self._init_components()

    def _init_components(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        if self.zep_client is None:
            from ..services.zep_client import ZepGraphClient
            self.zep_client = ZepGraphClient()

        if self.llm is None:
            from src.llms.llm import get_llm_by_type
            self.llm = get_llm_by_type("qwen")

        self.graph_id = self.zep_client.get_or_create_graph("news_graph")

        # ç¼“å­˜èŠ‚ç‚¹å’Œè¾¹æ•°æ®ç”¨äºå›¾æ¢ç´¢
        self._node_cache = {}
        self._edge_cache = {}

    def _analyze_query(self, question: str) -> Dict:
        """åˆ†ææŸ¥è¯¢ï¼Œæå–å®ä½“å’Œåˆ¤æ–­ç±»å‹"""
        try:
            prompt = QUERY_ANALYZER_PROMPT.format(question=question)
            response = self.llm.invoke([HumanMessage(content=prompt)])
            content = response.content if hasattr(response, 'content') else str(response)

            # æå– JSON
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())

            return {
                "query_type": "general",
                "entities": [],
                "keywords": [question],
                "intent": question
            }
        except Exception as e:
            logger.error(f"Query analysis failed: {e}")
            return {
                "query_type": "general",
                "entities": [],
                "keywords": [question],
                "intent": question
            }

    def _ensure_graph_loaded(self):
        """ç¡®ä¿å›¾æ•°æ®å·²åŠ è½½åˆ°å†…å­˜"""
        if not self._node_cache:
            try:
                graph_data = self.zep_client.get_graph_data(self.graph_id)
                # èŠ‚ç‚¹ç¼“å­˜: uuid -> node
                self._node_cache = {n.uuid: n for n in graph_data.nodes}
                # èŠ‚ç‚¹åç§°ç´¢å¼•: name -> uuid
                self._name_to_uuid = {}
                for n in graph_data.nodes:
                    self._name_to_uuid[n.name] = n.uuid
                    # ä¹Ÿç´¢å¼•éƒ¨åˆ†åŒ¹é…
                    for word in n.name.split():
                        if len(word) >= 2:
                            if word not in self._name_to_uuid:
                                self._name_to_uuid[word] = n.uuid

                # è¾¹ç¼“å­˜: é‚»æ¥è¡¨
                self._edge_cache = {}  # uuid -> [edges]
                self._all_edges = list(graph_data.edges)
                for e in graph_data.edges:
                    if e.source_node_uuid not in self._edge_cache:
                        self._edge_cache[e.source_node_uuid] = []
                    self._edge_cache[e.source_node_uuid].append(e)
                    if e.target_node_uuid not in self._edge_cache:
                        self._edge_cache[e.target_node_uuid] = []
                    self._edge_cache[e.target_node_uuid].append(e)

                logger.info(f"Graph loaded: {len(self._node_cache)} nodes, {len(self._all_edges)} edges")
            except Exception as e:
                logger.error(f"Failed to load graph: {e}")

    def _find_entity_node(self, entity_name: str):
        """æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°å®ä½“å¯¹åº”çš„èŠ‚ç‚¹"""
        # ç²¾ç¡®åŒ¹é…
        if entity_name in self._name_to_uuid:
            uuid = self._name_to_uuid[entity_name]
            return self._node_cache.get(uuid)

        # æ¨¡ç³ŠåŒ¹é…
        entity_lower = entity_name.lower()
        for node in self._node_cache.values():
            if entity_lower in node.name.lower() or node.name.lower() in entity_lower:
                return node
        return None

    def _local_search(self, entities: List[str], keywords: List[str]) -> List[SearchResult]:
        """
        LocalSearch: å®ä½“ä¸­å¿ƒå­å›¾æå–

        ç­–ç•¥ï¼š
        1. æ‰¾åˆ°å®ä½“å¯¹åº”çš„èŠ‚ç‚¹
        2. æå–è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰ç›´æ¥è¾¹ï¼ˆ1è·³é‚»å±…ï¼‰
        3. èšåˆå®ä½“å±æ€§å’Œå…³è”äº‹å®
        4. æŒ‰è¾¹çš„æ—¶é—´æ’åºï¼ˆæœ€æ–°ä¼˜å…ˆï¼‰
        """
        self._ensure_graph_loaded()
        results = []
        seen_facts = set()

        search_terms = entities + keywords[:2]

        for entity in search_terms[:5]:
            # 1. æ‰¾åˆ°å®ä½“èŠ‚ç‚¹
            node = self._find_entity_node(entity)
            if not node:
                continue

            # 2. è·å–æ‰€æœ‰ç›´æ¥å…³è”çš„è¾¹
            edges = self._edge_cache.get(node.uuid, [])

            # 3. æŒ‰æ—¶é—´æ’åºï¼ˆæ–°çš„åœ¨å‰ï¼‰
            sorted_edges = sorted(edges, key=lambda e: e.created_at or "", reverse=True)

            # 4. æå–äº‹å®
            for edge in sorted_edges[:20]:
                fact = edge.fact
                if not fact or fact in seen_facts:
                    continue
                seen_facts.add(fact)

                # æ‰¾å‡ºå…³è”çš„å¦ä¸€ä¸ªèŠ‚ç‚¹
                other_uuid = edge.target_node_uuid if edge.source_node_uuid == node.uuid else edge.source_node_uuid
                other_node = self._node_cache.get(other_uuid)
                other_name = other_node.name if other_node else "æœªçŸ¥"

                results.append(SearchResult(
                    strategy="LocalSearch",
                    content=fact,
                    score=1.0,
                    source_entity=node.name,
                    relation=f"{edge.name} â†’ {other_name}",
                    metadata={
                        "method": "subgraph_extraction",
                        "center_node": node.name,
                        "neighbor": other_name,
                        "edge_type": edge.name,
                        "created_at": edge.created_at
                    }
                ))

            # 5. æ·»åŠ èŠ‚ç‚¹æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
            if node.summary and node.summary not in seen_facts:
                seen_facts.add(node.summary)
                results.append(SearchResult(
                    strategy="LocalSearch",
                    content=f"[å®ä½“æ‘˜è¦] {node.name}: {node.summary}",
                    score=0.9,
                    source_entity=node.name,
                    relation="summary",
                    metadata={"method": "entity_summary"}
                ))

        return results[:20]

    def _global_search(self, question: str, keywords: List[str]) -> List[SearchResult]:
        """
        GlobalSearch: ç¤¾åŒºçº§å®è§‚æ£€ç´¢

        ç­–ç•¥ï¼š
        1. è¯­ä¹‰æœç´¢è·å–åˆå§‹ç»“æœ
        2. æå–é«˜é¢‘å‡ºç°çš„å®ä½“
        3. åŸºäºé«˜é¢‘å®ä½“æ‰©å±•æœç´¢ï¼ˆç¤¾åŒºå‘ç°çš„ç®€åŒ–ç‰ˆï¼‰
        4. èšåˆåŒä¸€ä¸»é¢˜çš„å¤šæ¡ä¿¡æ¯
        """
        self._ensure_graph_loaded()
        results = []
        seen_facts = set()
        entity_frequency = {}  # ç»Ÿè®¡å®ä½“å‡ºç°é¢‘ç‡

        # 1. ç¬¬ä¸€è½®ï¼šè¯­ä¹‰æœç´¢
        try:
            search_results = self.zep_client.search(question, self.graph_id, limit=20)

            for r in search_results:
                fact = r.get("fact", "")
                if fact and fact not in seen_facts:
                    seen_facts.add(fact)
                    results.append(SearchResult(
                        strategy="GlobalSearch",
                        content=fact,
                        score=r.get("score", 1.0),
                        relation=r.get("name", ""),
                        metadata={"method": "semantic_search", "round": 1}
                    ))

                    # ç»Ÿè®¡æ¶‰åŠçš„å®ä½“
                    source_uuid = r.get("source_node_uuid")
                    target_uuid = r.get("target_node_uuid")
                    for uuid in [source_uuid, target_uuid]:
                        if uuid and uuid in self._node_cache:
                            name = self._node_cache[uuid].name
                            entity_frequency[name] = entity_frequency.get(name, 0) + 1

        except Exception as e:
            logger.error(f"GlobalSearch semantic failed: {e}")

        # 2. æ‰¾å‡ºé«˜é¢‘å®ä½“ï¼ˆç¤¾åŒºä¸­å¿ƒï¼‰
        top_entities = sorted(entity_frequency.items(), key=lambda x: x[1], reverse=True)[:3]

        # 3. ç¬¬äºŒè½®ï¼šåŸºäºé«˜é¢‘å®ä½“æ‰©å±•
        for entity_name, freq in top_entities:
            if freq < 2:  # è‡³å°‘å‡ºç°2æ¬¡æ‰ç®—é«˜é¢‘
                continue

            node = self._find_entity_node(entity_name)
            if not node:
                continue

            edges = self._edge_cache.get(node.uuid, [])
            for edge in edges[:10]:
                fact = edge.fact
                if fact and fact not in seen_facts:
                    seen_facts.add(fact)
                    results.append(SearchResult(
                        strategy="GlobalSearch",
                        content=fact,
                        score=0.8,
                        source_entity=entity_name,
                        relation=edge.name,
                        metadata={
                            "method": "community_expansion",
                            "round": 2,
                            "hub_entity": entity_name,
                            "hub_frequency": freq
                        }
                    ))

        return results[:20]

    def _hybrid_search(self, entities: List[str], question: str) -> List[SearchResult]:
        """
        HybridSearch: åŒå‘æ£€ç´¢ + äº¤å‰éªŒè¯

        ç­–ç•¥ï¼š
        1. ä»é—®é¢˜å‡ºå‘ï¼šè¯­ä¹‰æœç´¢
        2. ä»å®ä½“å‡ºå‘ï¼šå›¾ç»“æ„æœç´¢
        3. å–äº¤é›†ï¼šåŒæ—¶è¢«ä¸¤ç§æ–¹æ³•æ‰¾åˆ°çš„ç»“æœå¯ä¿¡åº¦æ›´é«˜
        4. å¯¹ç»“æœè¿›è¡Œ reranking
        """
        self._ensure_graph_loaded()
        results = []

        # ç»“æœé›†åˆï¼Œç”¨äºäº¤å‰éªŒè¯
        semantic_facts = {}  # fact -> score
        graph_facts = {}     # fact -> score

        # 1. è¯­ä¹‰æœç´¢ï¼ˆä»é—®é¢˜å‡ºå‘ï¼‰
        try:
            search_results = self.zep_client.search(question, self.graph_id, limit=20)
            for r in search_results:
                fact = r.get("fact", "")
                if fact:
                    semantic_facts[fact] = r.get("score", 1.0)
        except Exception as e:
            logger.error(f"HybridSearch semantic failed: {e}")

        # 2. å›¾ç»“æ„æœç´¢ï¼ˆä»å®ä½“å‡ºå‘ï¼‰
        for entity in entities[:5]:
            node = self._find_entity_node(entity)
            if not node:
                continue

            edges = self._edge_cache.get(node.uuid, [])
            for edge in edges[:15]:
                fact = edge.fact
                if fact:
                    graph_facts[fact] = graph_facts.get(fact, 0) + 1.0

        # 3. äº¤å‰éªŒè¯ + Reranking
        all_facts = set(semantic_facts.keys()) | set(graph_facts.keys())

        for fact in all_facts:
            sem_score = semantic_facts.get(fact, 0)
            graph_score = graph_facts.get(fact, 0)

            # è®¡ç®—ç»¼åˆåˆ†æ•°
            if sem_score > 0 and graph_score > 0:
                # ä¸¤ç§æ–¹æ³•éƒ½æ‰¾åˆ°ï¼Œé«˜å¯ä¿¡åº¦
                final_score = (sem_score + graph_score) * 1.5
                method = "cross_validated"
            elif sem_score > 0:
                final_score = sem_score
                method = "semantic_only"
            else:
                final_score = graph_score * 0.8
                method = "graph_only"

            results.append(SearchResult(
                strategy="HybridSearch",
                content=fact,
                score=final_score,
                metadata={
                    "method": method,
                    "semantic_score": sem_score,
                    "graph_score": graph_score,
                    "cross_validated": sem_score > 0 and graph_score > 0
                }
            ))

        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x.score, reverse=True)
        return results[:20]

    def _chain_of_exploration(self, entities: List[str], max_hops: int = 2) -> List[SearchResult]:
        """
        ChainOfExploration: LLM å¼•å¯¼çš„å¤šè·³å›¾è°±æ¢ç´¢

        ç­–ç•¥ï¼š
        1. ä»ç§å­å®ä½“å‡ºå‘
        2. è·å–å€™é€‰é‚»å±…èŠ‚ç‚¹
        3. ç”¨ LLM åˆ¤æ–­å“ªäº›æ–¹å‘å€¼å¾—æ¢ç´¢ï¼ˆç›¸å…³æ€§è¯„åˆ†ï¼‰
        4. æ²¿é«˜åˆ†æ–¹å‘ç»§ç»­æ¢ç´¢
        5. è®°å½•æ¢ç´¢è·¯å¾„ï¼Œå½¢æˆæ¨ç†é“¾
        """
        self._ensure_graph_loaded()
        results = []
        seen_facts = set()
        explored_uuids: Set[str] = set()
        exploration_paths = []  # è®°å½•æ¢ç´¢è·¯å¾„

        for entity in entities[:3]:
            # 1. æ‰¾åˆ°ç§å­èŠ‚ç‚¹
            seed_node = self._find_entity_node(entity)
            if not seed_node:
                continue

            explored_uuids.add(seed_node.uuid)
            current_path = [seed_node.name]

            # 2. å¤šè·³æ¢ç´¢
            current_nodes = [(seed_node, current_path, 1.0)]  # (node, path, score)

            for hop in range(max_hops):
                next_nodes = []
                candidates_for_llm = []  # æ”¶é›†å€™é€‰èŠ‚ç‚¹è®© LLM è¯„åˆ†

                for current_node, path, path_score in current_nodes:
                    edges = self._edge_cache.get(current_node.uuid, [])

                    # æŒ‰æ—¶é—´æ’åºï¼Œä¼˜å…ˆæ¢ç´¢æ–°çš„è¾¹
                    sorted_edges = sorted(edges, key=lambda e: e.created_at or "", reverse=True)

                    for edge in sorted_edges[:15]:
                        # æ‰¾å‡ºé‚»å±…èŠ‚ç‚¹
                        neighbor_uuid = (edge.target_node_uuid
                                        if edge.source_node_uuid == current_node.uuid
                                        else edge.source_node_uuid)

                        if neighbor_uuid in explored_uuids:
                            continue

                        neighbor_node = self._node_cache.get(neighbor_uuid)
                        if not neighbor_node:
                            continue

                        # æ”¶é›†å€™é€‰
                        candidates_for_llm.append({
                            "node": neighbor_node,
                            "edge": edge,
                            "from_node": current_node,
                            "path": path,
                            "path_score": path_score
                        })

                # 3. å¦‚æœå€™é€‰å¤ªå¤šï¼Œç”¨å¯å‘å¼è§„åˆ™é¢„ç­›é€‰
                if len(candidates_for_llm) > 10:
                    # ä¼˜å…ˆé€‰æ‹©ï¼šæœ‰ fact çš„è¾¹ã€è¾ƒæ–°çš„è¾¹
                    candidates_for_llm = sorted(
                        candidates_for_llm,
                        key=lambda c: (
                            1 if c["edge"].fact else 0,
                            c["edge"].created_at or ""
                        ),
                        reverse=True
                    )[:10]

                # 4. å¯¹æ¯ä¸ªå€™é€‰ï¼Œè®¡ç®—æ¢ç´¢ä»·å€¼
                for candidate in candidates_for_llm:
                    edge = candidate["edge"]
                    neighbor = candidate["node"]
                    from_node = candidate["from_node"]
                    path = candidate["path"]
                    path_score = candidate["path_score"]

                    # è®°å½•äº‹å®
                    fact = edge.fact
                    if fact and fact not in seen_facts:
                        seen_facts.add(fact)

                        new_path = path + [f"--{edge.name}-->", neighbor.name]
                        path_str = " ".join(new_path)

                        # è·³æ•°è¡°å‡ + è·¯å¾„åˆ†æ•°
                        hop_decay = 1.0 / (hop + 1)
                        final_score = path_score * hop_decay

                        results.append(SearchResult(
                            strategy="ChainOfExploration",
                            content=fact,
                            score=final_score,
                            source_entity=entity,
                            relation=f"{from_node.name} --{edge.name}--> {neighbor.name}",
                            metadata={
                                "method": "guided_exploration",
                                "hop": hop + 1,
                                "path": path_str,
                                "edge_type": edge.name,
                                "created_at": edge.created_at
                            }
                        ))

                        exploration_paths.append(path_str)

                    # å‡†å¤‡ä¸‹ä¸€è·³
                    if neighbor.uuid not in explored_uuids:
                        explored_uuids.add(neighbor.uuid)
                        new_path = path + [f"--{edge.name}-->", neighbor.name]
                        # ä¼ é€’è¡°å‡åçš„åˆ†æ•°
                        next_nodes.append((neighbor, new_path, path_score * 0.7))

                # é™åˆ¶æ¯å±‚æ¢ç´¢çš„èŠ‚ç‚¹æ•°
                current_nodes = sorted(next_nodes, key=lambda x: x[2], reverse=True)[:8]

                if not current_nodes:
                    break

        # 5. æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x.score, reverse=True)

        # æ·»åŠ æ¢ç´¢æ‘˜è¦
        if exploration_paths:
            summary = f"æ¢ç´¢äº† {len(exploration_paths)} æ¡è·¯å¾„ï¼Œè¦†ç›– {len(explored_uuids)} ä¸ªèŠ‚ç‚¹"
            results.insert(0, SearchResult(
                strategy="ChainOfExploration",
                content=f"[æ¢ç´¢æ‘˜è¦] {summary}",
                score=0.5,
                metadata={"method": "exploration_summary", "paths_count": len(exploration_paths)}
            ))

        return results[:20]

    def _select_strategies(self, query_type: str, entities: List[str]) -> List[str]:
        """æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©ç­–ç•¥ç»„åˆ"""
        # é»˜è®¤æ‰€æœ‰ç­–ç•¥éƒ½ç”¨
        strategies = ["LocalSearch", "GlobalSearch", "HybridSearch", "ChainOfExploration"]

        if query_type == "entity" and entities:
            # å®ä½“æŸ¥è¯¢ä¼˜å…ˆ Local å’Œ Chain
            strategies = ["LocalSearch", "ChainOfExploration", "GlobalSearch", "HybridSearch"]
        elif query_type == "comparison" and len(entities) >= 2:
            # æ¯”è¾ƒæŸ¥è¯¢ä¼˜å…ˆ Hybrid
            strategies = ["HybridSearch", "LocalSearch", "ChainOfExploration", "GlobalSearch"]
        elif query_type == "timeline":
            # æ—¶é—´çº¿æŸ¥è¯¢ä¼˜å…ˆ Global
            strategies = ["GlobalSearch", "LocalSearch", "ChainOfExploration", "HybridSearch"]
        elif query_type == "causal":
            # å› æœæŸ¥è¯¢ä¼˜å…ˆ Chain
            strategies = ["ChainOfExploration", "HybridSearch", "LocalSearch", "GlobalSearch"]

        return strategies

    def _reflect_on_retrieval(self, question: str, results: Dict[str, List[SearchResult]]) -> Dict:
        """
        æ£€ç´¢ååæ€ï¼šè¯„ä¼°ç»“æœæ˜¯å¦è¶³å¤Ÿ
        """
        # ç”Ÿæˆç»“æœæ‘˜è¦
        total = sum(len(v) for v in results.values())
        summary_parts = []

        for strategy, items in results.items():
            if items:
                facts = [r.content[:80] for r in items[:5]]
                summary_parts.append(f"{strategy}: {len(items)}æ¡\n" + "\n".join(f"  - {f}" for f in facts))

        results_summary = "\n".join(summary_parts) if summary_parts else "æ— æ£€ç´¢ç»“æœ"

        try:
            prompt = RETRIEVAL_REFLECT_PROMPT.format(
                question=question,
                results_summary=results_summary
            )
            response = self.llm.invoke([HumanMessage(content=prompt)])
            content = response.content if hasattr(response, 'content') else str(response)

            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except Exception as e:
            logger.error(f"Retrieval reflection failed: {e}")

        # é»˜è®¤è®¤ä¸ºè¶³å¤Ÿ
        return {"sufficient": True, "reason": "é»˜è®¤é€šè¿‡", "missing": "", "suggestion": ""}

    def _reflect_on_answer(self, question: str, answer: str) -> Dict:
        """
        ç­”æ¡ˆåæ€ï¼šè¯„ä¼°ç­”æ¡ˆè´¨é‡
        """
        try:
            prompt = ANSWER_REFLECT_PROMPT.format(
                question=question,
                answer=answer[:1000]  # é™åˆ¶é•¿åº¦
            )
            response = self.llm.invoke([HumanMessage(content=prompt)])
            content = response.content if hasattr(response, 'content') else str(response)

            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except Exception as e:
            logger.error(f"Answer reflection failed: {e}")

        return {"quality": "acceptable", "issues": [], "suggestion": ""}

    def _supplementary_search(self, suggestion: str) -> List[SearchResult]:
        """
        è¡¥å……æ£€ç´¢
        """
        results = []
        try:
            search_results = self.zep_client.search(suggestion, self.graph_id, limit=10)
            seen = set()

            for r in search_results:
                fact = r.get("fact", "")
                if fact and fact not in seen:
                    seen.add(fact)
                    results.append(SearchResult(
                        strategy="SupplementarySearch",
                        content=fact,
                        score=r.get("score", 1.0),
                        relation=r.get("name", ""),
                        metadata={"type": "supplementary"}
                    ))
        except Exception as e:
            logger.error(f"Supplementary search failed: {e}")

        return results

    def _format_results_for_synthesis(self, results: Dict[str, List[SearchResult]]) -> Dict[str, str]:
        """æ ¼å¼åŒ–ç»“æœç”¨äºç»¼åˆ"""
        formatted = {}

        for strategy, items in results.items():
            if not items:
                formatted[strategy.lower().replace("search", "_results").replace("chainofexploration", "chain_results")] = "æ— ç›¸å…³ç»“æœ"
                continue

            lines = []
            for item in items[:10]:
                line = f"- {item.content}"
                if item.source_entity:
                    line += f" [å®ä½“: {item.source_entity}]"
                if item.metadata.get("hop"):
                    line += f" [è·³æ•°: {item.metadata['hop']}]"
                lines.append(line)

            key = strategy.lower()
            if "local" in key:
                formatted["local_results"] = "\n".join(lines)
            elif "global" in key:
                formatted["global_results"] = "\n".join(lines)
            elif "hybrid" in key:
                formatted["hybrid_results"] = "\n".join(lines)
            elif "chain" in key:
                formatted["chain_results"] = "\n".join(lines)

        # ç¡®ä¿æ‰€æœ‰keyéƒ½å­˜åœ¨
        for key in ["local_results", "global_results", "hybrid_results", "chain_results"]:
            if key not in formatted:
                formatted[key] = "æ— ç›¸å…³ç»“æœ"

        return formatted

    def run(self, question: str) -> Dict:
        """è¿è¡Œå¤šç­–ç•¥æ£€ç´¢"""
        # 1. åˆ†ææŸ¥è¯¢
        analysis = self._analyze_query(question)
        entities = analysis.get("entities", [])
        keywords = analysis.get("keywords", [question])
        query_type = analysis.get("query_type", "general")

        # 2. é€‰æ‹©ç­–ç•¥
        strategies = self._select_strategies(query_type, entities)

        # 3. å¹¶è¡Œæ‰§è¡Œæ£€ç´¢
        all_results = {}

        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {}

            if "LocalSearch" in strategies:
                futures[executor.submit(self._local_search, entities, keywords)] = "LocalSearch"
            if "GlobalSearch" in strategies:
                futures[executor.submit(self._global_search, question, keywords)] = "GlobalSearch"
            if "HybridSearch" in strategies:
                futures[executor.submit(self._hybrid_search, entities, question)] = "HybridSearch"
            if "ChainOfExploration" in strategies:
                futures[executor.submit(self._chain_of_exploration, entities)] = "ChainOfExploration"

            for future in as_completed(futures):
                strategy = futures[future]
                try:
                    all_results[strategy] = future.result()
                except Exception as e:
                    logger.error(f"{strategy} failed: {e}")
                    all_results[strategy] = []

        # 4. ç»¼åˆç”Ÿæˆ
        formatted = self._format_results_for_synthesis(all_results)
        prompt = SYNTHESIZER_PROMPT.format(question=question, **formatted)

        response = self.llm.invoke([HumanMessage(content=prompt)])
        answer = response.content if hasattr(response, 'content') else str(response)

        return {
            "answer": answer,
            "analysis": analysis,
            "strategies": strategies,
            "results": {k: len(v) for k, v in all_results.items()},
            "total_results": sum(len(v) for v in all_results.values())
        }

    def stream(self, question: str) -> Generator[Dict, None, None]:
        """
        æµå¼è¿è¡Œå¤šç­–ç•¥ Agentic RAG

        Yields:
            {"type": "log", ...}
            {"type": "chunk", "content": text}
            {"type": "done", "data": {...}}
        """
        all_logs = []

        def log(step: str, content: str, data: Any = None):
            entry = {
                "step": step,
                "content": content,
                "timestamp": time.strftime("%H:%M:%S"),
                "data": data
            }
            all_logs.append(entry)
            return entry

        # ===== Analyze é˜¶æ®µ =====
        yield {"type": "log", **log("ğŸ” Query Analysis", f"åˆ†æé—®é¢˜: {question}")}

        analysis = self._analyze_query(question)
        entities = analysis.get("entities", [])
        keywords = analysis.get("keywords", [question])
        query_type = analysis.get("query_type", "general")

        yield {"type": "log", **log(
            "ğŸ“Š åˆ†æç»“æœ",
            f"ç±»å‹: {query_type}, å®ä½“: {entities}",
            analysis
        )}

        # ===== Strategy Selection é˜¶æ®µ =====
        strategies = self._select_strategies(query_type, entities)

        yield {"type": "log", **log(
            "ğŸ¯ Strategy Selection",
            f"é€‰æ‹© {len(strategies)} ç§æ£€ç´¢ç­–ç•¥",
            {"strategies": strategies, "reason": f"åŸºäº {query_type} ç±»å‹é—®é¢˜ä¼˜åŒ–"}
        )}

        # ===== Execute é˜¶æ®µ - å¹¶è¡Œæ£€ç´¢ =====
        yield {"type": "log", **log(
            "âš¡ Execute",
            "å¹¶è¡Œæ‰§è¡Œå¤šç§æœç´¢ç­–ç•¥",
            {"parallel": True}
        )}

        all_results = {}
        strategy_status = {}

        # æ˜¾ç¤ºç­–ç•¥æ ‘
        strategy_tree = "å¹¶è¡Œæ‰§è¡Œå¤šç§æœç´¢ç­–ç•¥ï¼š\n"
        strategy_tree += "    â”œâ”€ [GraphRAG] LocalSearch: å®ä½“çº§ç²¾ç¡®æ£€ç´¢\n"
        strategy_tree += "    â”œâ”€ [GraphRAG] GlobalSearch: ç¤¾åŒºçº§å®è§‚æ£€ç´¢\n"
        strategy_tree += "    â”œâ”€ [GraphRAG] HybridSearch: åŒçº§èåˆæ£€ç´¢\n"
        strategy_tree += "    â””â”€ [GraphRAG] ChainOfExploration: å›¾è°±æ¢ç´¢"

        yield {"type": "log", **log("ğŸ“‹ æ‰§è¡Œè®¡åˆ’", strategy_tree)}

        # å¹¶è¡Œæ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {}
            start_times = {}

            if "LocalSearch" in strategies:
                futures[executor.submit(self._local_search, entities, keywords)] = "LocalSearch"
                start_times["LocalSearch"] = time.time()
            if "GlobalSearch" in strategies:
                futures[executor.submit(self._global_search, question, keywords)] = "GlobalSearch"
                start_times["GlobalSearch"] = time.time()
            if "HybridSearch" in strategies:
                futures[executor.submit(self._hybrid_search, entities, question)] = "HybridSearch"
                start_times["HybridSearch"] = time.time()
            if "ChainOfExploration" in strategies:
                futures[executor.submit(self._chain_of_exploration, entities)] = "ChainOfExploration"
                start_times["ChainOfExploration"] = time.time()

            for future in as_completed(futures):
                strategy = futures[future]
                elapsed = int((time.time() - start_times[strategy]) * 1000)

                try:
                    results = future.result()
                    all_results[strategy] = results

                    # æå–é¢„è§ˆ
                    preview = [r.content[:60] + "..." for r in results[:3]]

                    yield {"type": "log", **log(
                        f"âœ“ {strategy}",
                        f"æ‰¾åˆ° {len(results)} æ¡ç»“æœ ({elapsed}ms)",
                        {"count": len(results), "preview": preview}
                    )}

                except Exception as e:
                    logger.error(f"{strategy} failed: {e}")
                    all_results[strategy] = []
                    yield {"type": "log", **log(
                        f"âœ— {strategy}",
                        f"æ‰§è¡Œå¤±è´¥: {str(e)[:50]}",
                        {"error": str(e)}
                    )}

        # ===== Merge é˜¶æ®µ =====
        total_results = sum(len(v) for v in all_results.values())
        yield {"type": "log", **log(
            "ğŸ”€ Results Merge",
            f"åˆå¹¶ {total_results} æ¡æ£€ç´¢ç»“æœï¼Œå»é‡å¹¶æ’åº"
        )}

        # ===== Reflect é˜¶æ®µ 1: æ£€ç´¢ç»“æœåæ€ =====
        yield {"type": "log", **log("ğŸ”„ Reflect (Retrieval)", "è¯„ä¼°æ£€ç´¢ç»“æœæ˜¯å¦è¶³å¤Ÿ...")}

        retrieval_reflect = self._reflect_on_retrieval(question, all_results)
        is_sufficient = retrieval_reflect.get("sufficient", True)

        if is_sufficient:
            yield {"type": "log", **log(
                "âœ“ æ£€ç´¢å……åˆ†",
                retrieval_reflect.get("reason", "ç»“æœè¶³å¤Ÿå›ç­”é—®é¢˜")
            )}
        else:
            yield {"type": "log", **log(
                "âš ï¸ æ£€ç´¢ä¸è¶³",
                f"{retrieval_reflect.get('reason', '')}ï¼Œç¼ºå°‘: {retrieval_reflect.get('missing', '')}"
            )}

            # è¡¥å……æ£€ç´¢
            suggestion = retrieval_reflect.get("suggestion", "")
            if suggestion:
                yield {"type": "log", **log(
                    "ğŸ” è¡¥å……æ£€ç´¢",
                    f"å…³é”®è¯: {suggestion}"
                )}

                supplementary = self._supplementary_search(suggestion)
                if supplementary:
                    all_results["SupplementarySearch"] = supplementary
                    total_results += len(supplementary)

                    yield {"type": "log", **log(
                        "âœ“ è¡¥å……å®Œæˆ",
                        f"æ–°å¢ {len(supplementary)} æ¡ç»“æœ",
                        {"preview": [r.content[:50] for r in supplementary[:3]]}
                    )}

        # ===== Synthesize é˜¶æ®µ =====
        yield {"type": "log", **log("ğŸ§  Synthesize", "åŸºäºå¤šç­–ç•¥ç»“æœç»¼åˆç”Ÿæˆå›ç­”...")}

        formatted = self._format_results_for_synthesis(all_results)
        prompt = SYNTHESIZER_PROMPT.format(question=question, **formatted)

        full_answer = ""
        try:
            for chunk in self.llm.stream([HumanMessage(content=prompt)]):
                content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                if content:
                    full_answer += content
                    yield {"type": "chunk", "content": content}
        except Exception as e:
            error_msg = str(e)
            if "inappropriate" in error_msg.lower() or "content" in error_msg.lower():
                full_answer = "æ£€ç´¢åˆ°çš„æ–°é—»å†…å®¹æ¶‰åŠæ•æ„Ÿè¯é¢˜ï¼Œæ— æ³•ç”Ÿæˆå®Œæ•´å›ç­”ã€‚å»ºè®®å°è¯•æ›´å…·ä½“çš„é—®é¢˜ï¼Œæˆ–æŸ¥çœ‹åŸå§‹æ£€ç´¢ç»“æœã€‚"
                yield {"type": "chunk", "content": full_answer}
            else:
                raise e

        # ===== Reflect é˜¶æ®µ 2: ç­”æ¡ˆè´¨é‡åæ€ =====
        yield {"type": "log", **log("ğŸ”„ Reflect (Answer)", "è¯„ä¼°ç­”æ¡ˆè´¨é‡...")}

        answer_reflect = self._reflect_on_answer(question, full_answer)
        quality = answer_reflect.get("quality", "acceptable")
        issues = answer_reflect.get("issues", [])

        if quality == "good":
            yield {"type": "log", **log("âœ“ ç­”æ¡ˆè´¨é‡è‰¯å¥½", "å›ç­”ç›´æ¥ä¸”å®Œæ•´")}
        elif quality == "acceptable":
            yield {"type": "log", **log("âœ“ ç­”æ¡ˆè´¨é‡å¯æ¥å—", "å›ç­”åŸºæœ¬æ»¡è¶³éœ€æ±‚")}
        else:
            issue_str = "ã€".join(issues) if issues else "è´¨é‡è¾ƒå·®"
            yield {"type": "log", **log(
                "âš ï¸ ç­”æ¡ˆè´¨é‡å¾…æ”¹è¿›",
                f"é—®é¢˜: {issue_str}",
                {"suggestion": answer_reflect.get("suggestion", "")}
            )}

        yield {"type": "log", **log("âœ… Complete", f"ç”Ÿæˆäº† {len(full_answer)} å­—çš„å›ç­”")}

        yield {
            "type": "done",
            "data": {
                "answer": full_answer,
                "analysis": analysis,
                "strategies": strategies,
                "results": {k: len(v) for k, v in all_results.items()},
                "total_results": total_results,
                "retrieval_reflect": retrieval_reflect,
                "answer_reflect": answer_reflect,
                "logs": all_logs
            }
        }
